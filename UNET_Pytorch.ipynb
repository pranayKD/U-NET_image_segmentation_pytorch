{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNET_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuyoooSC1Ib",
        "colab_type": "text"
      },
      "source": [
        "### Set up google drive and unzip train data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHb3jn_zSlze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up google drive in google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU3Xw67U8vOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip training data from drive\n",
        "\n",
        "!unzip -q 'drive/My Drive/VOCdevkit.zip'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah1ZPcv0DEKJ",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlfwqR7Y87Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image,ImageFilter, ImageEnhance\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m5SUCwcDUqd",
        "colab_type": "text"
      },
      "source": [
        "### Handle Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjH76sVFBD3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "92a2c43b-9a2d-4086-b65c-675078f30dd9"
      },
      "source": [
        "img_dir = 'VOCdevkit/VOC2007/JPEGImages/'\n",
        "out_dir = 'VOCdevkit/VOC2007/SegmentationClass/'\n",
        "\n",
        "img_size = (128,128)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFGsYlhDiD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b9787238-3eb5-4970-f38c-eb688e1579d1"
      },
      "source": [
        "all_labels = []\n",
        "\n",
        "for img in os.listdir(out_dir):\n",
        "    img = Image.open(out_dir + img)\n",
        "    img = np.array(img)\n",
        "    all_labels += np.unique(img).tolist()\n",
        "\n",
        "\n",
        "print(set(all_labels))\n",
        "\n",
        "num_class = 3 # background and boundary and object \n",
        "print(\"number of classes excluding background and boundary - \", num_class)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 255}\n",
            "number of classes excluding background and boundary -  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGldQOe5DiFt",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b58As2b6Vdsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Random blur on training image\n",
        "def random_blur(img):\n",
        "    if random.random() < 0.4:\n",
        "        return img\n",
        "    rad = random.choice([0.5,1,1.5,2])\n",
        "    img = img.filter(ImageFilter.BoxBlur(radius=rad))\n",
        "    return img\n",
        "\n",
        "# Random brightness, contrast, satutration and hue\n",
        "def random_color(img):\n",
        "  if random.random() < 0.3:\n",
        "    return img\n",
        "\n",
        "  img = transforms.ColorJitter(brightness=(0.5,2.0), contrast=(0.5,2.0), saturation=(0.5,2.0), hue=(-0.25,0.25))(img)\n",
        "  return img\n",
        "\n",
        "\n",
        "# Random horizontal flip\n",
        "def random_flip(img, out):\n",
        "  if random.random() < 0.5:\n",
        "    return img, out\n",
        "\n",
        "  img = transforms.RandomHorizontalFlip(p=1)(img)\n",
        "  out = transforms.RandomHorizontalFlip(p=1)(out)\n",
        "\n",
        "  return img, out\n",
        "\n",
        "# Random crop on image\n",
        "def random_crop(img, out):\n",
        "  if random.random() < 0.3:\n",
        "    return img,out\n",
        "\n",
        "  width, height = img.size\n",
        "  select_w = random.uniform(0.7*width, width)\n",
        "  select_h = random.uniform(0.7*height, height)\n",
        "\n",
        "  start_x = random.uniform(0,width - select_w)\n",
        "  start_y = random.uniform(0,height - select_h)\n",
        "\n",
        "  left = start_x\n",
        "  upper = start_y\n",
        "  right = start_x + select_w\n",
        "  bottom = start_y + select_h\n",
        "\n",
        "  return img.crop((left, upper, right, bottom)) ,out.crop((left, upper, right, bottom))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u40lTQ3vDlsz",
        "colab_type": "text"
      },
      "source": [
        "### Dataset define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqvv68LH9XT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pascal_voc_data(Dataset):\n",
        "    def __init__(self, img_dir, out_dir,type_list, isTrain, transform):\n",
        "        super().__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.out_dir = out_dir\n",
        "        self.type_list = type_list\n",
        "        self.isTrain = isTrain\n",
        "        self.transform = transform\n",
        "\n",
        "        self.img_names = []\n",
        "        self.out_names = []\n",
        "\n",
        "        for img in sorted(os.listdir(img_dir)):\n",
        "            if img[:-4] in self.type_list:\n",
        "                self.img_names.append(img)\n",
        "\n",
        "        for img in sorted(os.listdir(out_dir)):\n",
        "            if img[:-4] in self.type_list:\n",
        "                self.out_names.append(img)\n",
        "\n",
        "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
        "        self.out_names = [os.path.join(out_dir, out_name) for out_name in self.out_names]\n",
        "\n",
        "        assert (len(self.img_names) == len(self.out_names)), \"Error - Input and output image size is different\"\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        out_name = self.out_names[idx]\n",
        "\n",
        "        \n",
        "        img = Image.open(img_name)\n",
        "        out = Image.open(out_name)\n",
        "\n",
        "        if self.isTrain:\n",
        "            img = random_blur(img)\n",
        "            img = random_color(img)\n",
        "            img,out = random_flip(img,out)\n",
        "            img,out = random_crop(img,out)\n",
        "\n",
        "        img = self.transform(img)\n",
        "\n",
        "        out = np.array(transforms.Resize(img_size)(out))\n",
        "        out[(out != 0 ) & (out != 255)] = 1   #Foreground is 1, background is 0\n",
        "        out[(out == 255)] = 2 # Boundary is 2\n",
        "        out = torch.IntTensor(out)\n",
        "\n",
        "        return img,out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UU5EjMD_6C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "While using pretrained models - \n",
        "Pytorch torchvision documentation - https://pytorch.org/docs/master/torchvision/models.html\n",
        "The images have to be loaded in to a range of [0, 1] and then \n",
        "normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
        "'''\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(img_size),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "   std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIwUhUGBDpRX",
        "colab_type": "text"
      },
      "source": [
        "### Split into train and valid "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn32qMU09ux7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0d4282db-aa69-4216-a1ff-229c082465a2"
      },
      "source": [
        "file = open('drive/My Drive/val.txt', \"r\")\n",
        "valid_images = file.read().split('\\n')\n",
        "valid_images = valid_images[:-1]\n",
        "\n",
        "train_images = []\n",
        "for img in os.listdir('VOCdevkit/VOC2007/JPEGImages/'):\n",
        "  if img[:-4] in valid_images:\n",
        "    continue\n",
        "  train_images.append(img[:-4])\n",
        "\n",
        "print('train_images',len(train_images))\n",
        "print('valid_images',len(valid_images))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_images 209\n",
            "valid_images 213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDXCMJ7q9_8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = []\n",
        "\n",
        "for img in train_images:\n",
        "    img = Image.open(out_dir + img + '.png')\n",
        "    img = np.array(img)\n",
        "    train_labels += np.unique(img).tolist()\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "print(np.unique(train_labels, return_counts=True)[0])\n",
        "print(np.unique(train_labels, return_counts=True)[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4LQNQtfRgja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_labels = []\n",
        "\n",
        "for img in valid_images:\n",
        "    img = Image.open(out_dir + img + '.png')\n",
        "    img = np.array(img)\n",
        "    valid_labels += np.unique(img).tolist()\n",
        "\n",
        "valid_labels = np.array(valid_labels)\n",
        "print(np.unique(valid_labels, return_counts=True)[0])\n",
        "print(np.unique(valid_labels, return_counts=True)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDxrQaeMCZcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = pascal_voc_data(img_dir, out_dir, train_images,True,transform)\n",
        "valid_dataset = pascal_voc_data(img_dir, out_dir, valid_images, False, transform)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztsGjlQgDy1R",
        "colab_type": "text"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHKI_Q1UC5ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualise(img, out):\n",
        "    fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    transform_img = inv_normalize(img[0]).permute(1,2,0).to('cpu').numpy()\n",
        "    transform_img = transform_img.copy()\n",
        "    ax1.imshow(transform_img)\n",
        "    ax2.imshow(out[0].to('cpu').numpy())\n",
        "    ax3.imshow(transform_img)\n",
        "    ax3.imshow(out[0].to('cpu').numpy(), alpha=0.5)\n",
        "    plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seY8D3QaD2dX",
        "colab_type": "text"
      },
      "source": [
        "### Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifPM-ySTghQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "\n",
        "        # Input - 3*512*512\n",
        "        self.conv1 = nn.Conv2d(3,64,3,1,1)  #64*512*512\n",
        "        self.conv2 = nn.Conv2d(64,64,3,1,1) #64*512*512\n",
        "        self.pool1 = nn.MaxPool2d(2,2) #64*256*256\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64,128,3,1,1) #128*256*256\n",
        "        self.conv4 = nn.Conv2d(128,128,3,1,1) #128*256*256\n",
        "        self.pool2 = nn.MaxPool2d(2,2) #128*128*128\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128,256,3,1,1) #256*128*128\n",
        "        self.conv6 = nn.Conv2d(256,256,3,1,1) #256*128*128\n",
        "        self.pool3 = nn.MaxPool2d(2,2) #256*64*64\n",
        "        self.drop3 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256,512,3,1,1) #512*64*64\n",
        "        self.conv8 = nn.Conv2d(512,512,3,1,1) #512*64*64\n",
        "        self.pool4 = nn.MaxPool2d(2,2)  #512*32*32\n",
        "\n",
        "        self.conv9 = nn.Conv2d(512,1024,3,1,1) #1024*32*32\n",
        "        self.conv10 = nn.Conv2d(1024,1024,3,1,1) #1024*32*32\n",
        "        self.drop5 = nn.Dropout(p=0.3)\n",
        "\n",
        "        # Decoder\n",
        "        \n",
        "        self.TConv1 = nn.ConvTranspose2d(1024,512,2,2,0) #512*64*64\n",
        "        #1024*64*64 - torch.cat conv8\n",
        "\n",
        "        self.conv11 = nn.Conv2d(1024,512,3,1,1) #512*64*64\n",
        "        self.conv12 = nn.Conv2d(512,512,3,1,1) #512*64*64\n",
        "\n",
        "        self.TConv2 = nn.ConvTranspose2d(512,256,2,2,0) #256*128*128\n",
        "        # 512*128*128 - torch.cat conv6\n",
        "        self.conv13 = nn.Conv2d(512,256,3,1,1) #256*128*128\n",
        "        self.conv14 = nn.Conv2d(256,256,3,1,1) #256*128*128\n",
        "        self.drop7 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.TConv3 = nn.ConvTranspose2d(256,128,2,2,0) #128*256*256\n",
        "        # 256*256*256 - torch.cat conv4\n",
        "        self.conv15 = nn.Conv2d(256,128,3,1,1) #128*256*256\n",
        "        self.conv16 = nn.Conv2d(128,128,3,1,1) #128*256*256\n",
        "\n",
        "        self.TConv4 = nn.ConvTranspose2d(128,64,2,2,0) #64*512*512\n",
        "        # 128*512*512 - torch.cat conv2\n",
        "        self.conv17 = nn.Conv2d(128,64,3,1,1) #64*512*512\n",
        "        self.conv18 = nn.Conv2d(64,64,3,1,1) #64*512*512\n",
        "        self.conv19 = nn.Conv2d(64,num_class,1,1,0) #3*512*512\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out1 = F.relu(self.conv2((F.relu(self.conv1(x)))))\n",
        "\n",
        "        in2 = self.pool1(out1)\n",
        "        out2 = F.relu(self.conv4(self.drop3(F.relu(self.conv3(in2)))))\n",
        "\n",
        "        in3 = self.pool2(out2)\n",
        "        out3 = F.relu(self.conv6((F.relu(self.conv5(in3)))))\n",
        "\n",
        "        in4 = self.pool3(out3)\n",
        "        out4 = F.relu(self.conv8((F.relu(self.conv7(in4)))))\n",
        "\n",
        "        in5 = self.pool4(out4)\n",
        "        in5 = F.relu(self.conv10(self.drop5(F.relu(self.conv9(in5)))))\n",
        "\n",
        "        dout4 = F.relu(self.TConv1(in5))\n",
        "        din4 = torch.cat([out4,dout4], dim=1)\n",
        "        dout3 = F.relu(self.conv12((F.relu(self.conv11(din4)))))\n",
        "\n",
        "        dout3 = F.relu(self.TConv2(dout3))\n",
        "        din3 = torch.cat([out3,dout3],dim=1)\n",
        "        dout2 = F.relu(self.conv14(self.drop7(F.relu(self.conv13(din3)))))\n",
        "\n",
        "        dout2 = F.relu(self.TConv3(dout2))\n",
        "        din2 = torch.cat([out2,dout2], dim=1) \n",
        "        dout1 = F.relu(self.conv16((F.relu(self.conv15(din2)))))\n",
        "\n",
        "        dout1 = F.relu(self.TConv4(dout1))\n",
        "        din1 = torch.cat([out1,dout1], dim=1)\n",
        "        \n",
        "\n",
        "        out = self.conv19(F.relu(self.conv18((F.relu(self.conv17(din1))))))\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNiNq8UwD5xI",
        "colab_type": "text"
      },
      "source": [
        "### Dice definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aBqSGWFbCsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dice_per_img(preds, targets, eps = 1e-6):\n",
        "    if ((preds.sum() + targets.sum()) == 0):\n",
        "        return 1.0\n",
        "\n",
        "    num = 2.0  * ((preds*targets).sum() + eps)\n",
        "    den = (preds.sum() + targets.sum() + eps)\n",
        "\n",
        "    return (num/den)\n",
        "\n",
        "\n",
        "def get_dice(preds, targets_inp, eps = 1e-6):\n",
        "    targets = targets_inp.clone()\n",
        "    #preds = batch x nChannels x Height x Width\n",
        "    #Target = batch x Height x Width\n",
        "\n",
        "    targets = (targets[...,None] == torch.arange(num_class).to(device)).int() # batch x Height x Width x nChannels\n",
        "    targets = targets.permute(0,3,1,2)  # batch x nChannels x Height x Width \n",
        "\n",
        "    dice = 0\n",
        "    for cls in range(preds.shape[0]):\n",
        "        cls_pred = preds[cls,:,:,:]\n",
        "        cls_targ = targets[cls,:,:,:]\n",
        "\n",
        "        dice +=  get_dice_per_img(cls_pred, cls_targ)\n",
        "\n",
        "    return dice/(1.0 * preds.shape[0])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfpWa0W5D8Sq",
        "colab_type": "text"
      },
      "source": [
        "### Create new checkpoint or load from saved checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALV-O70IVM-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_model = ''\n",
        "\n",
        "net = UNET().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "batch_size = 4\n",
        "avg_loss_idx = 10\n",
        "best_valid_score = 100000\n",
        "epoch_start = 0\n",
        "loss_hist = []\n",
        "valid_hist = []\n",
        "dice_hist = []\n",
        "\n",
        "if load_model != '':\n",
        "    print('loading model ... ')\n",
        "    checkpoint = torch.load(load_model, map_location=device)\n",
        "    net.load_state_dict(checkpoint['net_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    valid_hist = checkpoint['valid_hist']\n",
        "    dice_hist = checkpoint['dice_hist']\n",
        "    best_valid_score = checkpoint['best_valid_score']\n",
        "    epoch_start =checkpoint['epoch_start']\n",
        "    \n",
        "    net.train()\n",
        "    print('model loaded ...' )\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvb0zHGEEOuB",
        "colab_type": "text"
      },
      "source": [
        "### Training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cbkkNLBFE2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for epoch in range(epoch_start,epoch_start+500):\n",
        "    net.train()\n",
        "    print('epoch -- ',epoch)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    running_train_loss = 0\n",
        "    for train_idx, train_data in enumerate(train_loader):\n",
        "        \n",
        "        img, label = train_data\n",
        "\n",
        "        pred = net(img.to(device))\n",
        "\n",
        "        loss = criterion(pred,label.to(device).long())\n",
        "\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss\n",
        "\n",
        "        \n",
        "        if((train_idx+1) % (avg_loss_idx) == 0):\n",
        "            loss_hist.append(running_train_loss/avg_loss_idx)\n",
        "            print(str(train_idx) + ' -- ' + str((running_train_loss/avg_loss_idx).item()))\n",
        "            running_train_loss = 0\n",
        "        \n",
        "\n",
        "    visualise(img,torch.argmax(pred,axis=1))\n",
        "    visualise(img,label)\n",
        "    plt.plot(loss_hist)\n",
        "    plt.show()\n",
        "\n",
        "    print('--- Evaluate Valid Data ---')\n",
        "\n",
        "    net.eval()\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "    total_valid_loss = 0\n",
        "    for valid_idx, valid_data in enumerate(valid_loader):\n",
        "        valid_img, valid_label = valid_data\n",
        "\n",
        "        with torch.no_grad():\n",
        "            valid_pred = net(valid_img.to(device))\n",
        "\n",
        "        loss = criterion(valid_pred, valid_label.to(device).long())\n",
        "\n",
        "        total_valid_loss += loss\n",
        "\n",
        "    valid_loss = total_valid_loss/valid_idx\n",
        "\n",
        "    valid_hist.append(valid_loss)\n",
        "\n",
        "    visualise(valid_img,torch.argmax(valid_pred,axis=1))\n",
        "    plt.plot(valid_hist)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print('--- Save Model ---')\n",
        "\n",
        "    PATH = 'drive/My Drive/saved_models/current.pt'\n",
        "    net.train()\n",
        "    torch.save({\n",
        "        'net_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss_hist':loss_hist,\n",
        "        'valid_hist':valid_hist,\n",
        "        'dice_hist':dice_hist,\n",
        "        'best_valid_score':best_valid_score,\n",
        "        'epoch_start':epoch\n",
        "      }, PATH)\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    if valid_loss < best_valid_score:\n",
        "        print('--- Save Best Model ---')\n",
        "        best_valid_score = valid_loss\n",
        "        net.train()\n",
        "        PATH = 'drive/My Drive/saved_models/best.pt'\n",
        "        torch.save({\n",
        "          'net_state_dict': net.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss_hist':loss_hist,\n",
        "          'valid_hist':valid_hist,\n",
        "          'dice_hist':dice_hist,\n",
        "          'best_valid_score':best_valid_score,\n",
        "          'epoch_start':epoch\n",
        "        }, PATH)\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}